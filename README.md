ğŸ“˜ Chat with Your PDF Using AI â€“ Python + LangChain + Local LLM (Flask UI)

This project allows you to chat with any PDF file locally using Python, LangChain, FAISS, and a lightweight local LLM (TinyLlama).
You can upload a PDF, ask questions, and get meaningful responses â€” all offline, without using any external APIs.

ğŸ¯ Features

Upload and preview PDF files

Ask questions and get accurate, contextual answers

Works fully offline with a local LLM

Real-time typing effect for replies

Clear chat option

Simple Flask web interface

ğŸ› ï¸ Tech Stack
Backend

Python 3.10+

Flask

LangChain

TinyLlama (local model with ctransformers)

FAISS (vector search)

Sentence Transformers

PyMuPDF

Frontend

HTML

CSS

JavaScript

ğŸ“¦ Requirements (requirements.txt)
flask
langchain
faiss-cpu
sentence-transformers
PyMuPDF
ctransformers

ğŸš€ Installation & Setup
1ï¸âƒ£ Clone or Download the Project
git clone https://github.com/your-username/chat-with-pdf-ai.git
cd chat-with-pdf-ai

2ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

3ï¸âƒ£ Add the Local LLM Model

Download the TinyLlama GGUF model:

tinyllama-1.1b-chat-v1.0.Q4_0.gguf

Place it inside the models/ folder:

project/
â””â”€â”€ models/
      tinyllama-1.1b-chat-v1.0.Q4_0.gguf

4ï¸âƒ£ Run the Flask App
python app.py

5ï¸âƒ£ Open in Browser
http://127.0.0.1:5000

ğŸ“ Folder Structure
project/
â”œâ”€â”€ app.py
â”œâ”€â”€ uploads/
â”œâ”€â”€ models/
â”‚   â””â”€â”€ tinyllama-1.1b-chat-v1.0.Q4_0.gguf
â”œâ”€â”€ static/
â”‚   â””â”€â”€ styles.css
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html
â””â”€â”€ requirements.txt

ğŸ’¡ Why I Built This Project

By doing this project, I am learning how Flask works, how local LLMs run, and how to connect them with LangChain, embeddings, and vector search.
