ğŸ“˜ Chat with Your PDF Using AI â€“ Python + LangChain + Local LLM (Flask UI)

This project lets you chat with any PDF file locally using Python, LangChain, FAISS, and a lightweight TinyLlama LLM.
You can upload a PDF, ask questions, and get meaningful answers â€” fully offline and without depending on external APIs.

ğŸ¯ Features

Upload and extract text from PDF files

Ask questions and receive context-based answers

Works completely offline with a local LLM

Typing animation for responses

Option to clear chat

Simple Flask-based web interface

ğŸ› ï¸ Tech Stack
Backend

Python 3.10+

Flask

LangChain

TinyLlama (local GGUF model using ctransformers)

FAISS for vector search

Sentence Transformers

PyMuPDF for PDF reading

Frontend

HTML

CSS

JavaScript

ğŸ“¦ Requirements (requirements.txt)
flask
langchain
faiss-cpu
sentence-transformers
PyMuPDF
ctransformers

ğŸš€ Setup Instructions
1ï¸âƒ£ Install the project dependencies

Create a virtual environment (optional) and install the required packages:

pip install -r requirements.txt

2ï¸âƒ£ Add the Local LLM Model

Place this model file inside the models folder:

tinyllama-1.1b-chat-v1.0.Q4_0.gguf

Folder structure example:

models/
   tinyllama-1.1b-chat-v1.0.Q4_0.gguf

3ï¸âƒ£ Run the Flask Application
python app.py

4ï¸âƒ£ Open in your browser
http://127.0.0.1:5000

ğŸ“ Folder Structure
project/
â”œâ”€â”€ app.py
â”œâ”€â”€ uploads/
â”œâ”€â”€ models/
â”‚   â””â”€â”€ tinyllama-1.1b-chat-v1.0.Q4_0.gguf
â”œâ”€â”€ static/
â”‚   â””â”€â”€ styles.css
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html
â””â”€â”€ requirements.txt

ğŸ’¡ Personal Note

I built this project to learn how Flask works, how to use local LLMs, and how to connect PDFs, embeddings, and vector search using LangChain.
